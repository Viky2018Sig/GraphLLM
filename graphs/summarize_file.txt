#
# This graph sends a query to the inference server (llama.cpp)
# There is a hidden node called _C that contains the command line arguments
# The command line argument is used to complete the query.

# example usage
# python3 exec.py graphs/summarize_file.txt test/wikipedia_summary.txt
    
T1:
    init: templates/summarize_full.txt
    conf: {"temperature":0.01, "top_k":20,"print_prompt":false, "n_predict":4096}
    exec: _C[1]

