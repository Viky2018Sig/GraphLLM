###

client_name: llama_cpp

###
llama_cpp:
    host: "localhost"
    port: 8080

#groq:
#    api_key: XXX
